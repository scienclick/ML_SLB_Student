{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-fund-d3",
      "display_name": "Python (env Fund-d3)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "ashamsa@slb.com",
    "createdOn": 1663181300495,
    "tags": [],
    "customFields": {},
    "modifiedBy": "dbecerra6@slb.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel `Fund-d3`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Day 3 - Tutorial 1\n\n## PART 1: Clustering Analysis\n\nClustering refers to grouping similar data points together, based on their attributes or features\n\nThis tutorial will be divided into two parts:\n\nIn the first part, we will explore various clustering methods using log data from a single well\n\nIn the second part, we will use the production dataset to generate a model that predicts the production trend for a well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Import Required libraries"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Import the requiered libraries\n\nimport pandas as pd\nfrom pandas import DataFrame\nfrom numpy import nan as NA\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\n\nimport dataiku\nfrom dataiku import pandasutils as pdu"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Import and Explore the Dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Load the dataset \u0027w5\u0027 from Dataiku \n\nmydataset \u003d dataiku.Dataset(\"w5\")\nw5_logs \u003d mydataset.get_dataframe()\n\nw5_logs"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate descriptive statistics of the log data\n\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Preprocessing data -  Standardization\n\nClustering models are distance based algorithms, in order to measure similarities between observations and form clusters they use a distance metric. So, features with high ranges will have a bigger influence on the clustering. Therefore, it\u0027s a good practice to scale the data before applying clustering analysis.\n\nStandardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use the StandardScaler() and scaler.transform() functions to standardize each column of the w5_logs dataset\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler \u003d StandardScaler().fit(w5_logs)\n\n\n# The scaler.transform() function subtracts the mean and divides the result by the std of each column\n\nw5_logs_s \u003d scaler.transform(w5_logs)\n\n\n# The return value after the transformation is a numpy array, let\u0027s explore it\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert the numpy array into a pandas DataFrame, call it \u0027w5_logs_scaled\u0027\n\nw5_logs_scaled \u003d pd.DataFrame(w5_logs_s,\n                              columns\u003d[\u0027DEPTH\u0027,\u0027Pressure\u0027, \u0027AI\u0027, \u0027VpVs\u0027, \u0027SW\u0027, \u0027phid\u0027, \u0027vcl\u0027])\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print the standardized dataset and assign \u0027DEPTH\u0027 as index\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Clustering Analysis Using K-Means\n\nThe KMeans algorithm clusters data by trying to separate samples in n groups of equal variance"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Extract 3 clusters of data using K-Means. Use the raw data (Not Scaled Data)\n\nfrom sklearn.cluster import KMeans\n\nkmeans \u003d KMeans(n_clusters\u003d3)\n\npred \u003d kmeans.fit_predict(w5_logs[[\u0027VpVs\u0027, \u0027AI\u0027]]) \n\n\n# Display the VpVs versus AI using the previously run K-means algorithm\n\nfig, (ax1, ax2) \u003d plt.subplots(1, 2, figsize\u003d(10, 5))\n\nplt.subplot(1, 2, 1)\n\nax1 \u003d plt.scatter(w5_logs[\"AI\"],w5_logs[\"VpVs\"], color\u003d\u0027gray\u0027, edgecolor\u003d\u0027black\u0027, lw\u003d0.3, s\u003d30)\nplt.xlabel(\"AI\",fontsize\u003d14)\nplt.ylabel(r\u0027$\\frac{Vp}{Vs}$\u0027,fontsize\u003d14)\nplt.title(\u0027Raw Data\u0027,fontsize\u003d14)\n\n\nplt.subplot(1, 2, 2)\n\nax2 \u003d plt.scatter(w5_logs[\"AI\"],w5_logs[\"VpVs\"], c\u003dpred, edgecolor\u003d\u0027black\u0027, lw\u003d0.3, s\u003d30,cmap\u003d\"prism\")\nplt.xlabel(\"AI\",fontsize\u003d14)\nplt.ylabel(r\u0027$\\frac{Vp}{Vs}$\u0027,fontsize\u003d14)\nplt.title(\u0027K-Means Data Not Scaled\u0027,fontsize\u003d14)\nplt.tight_layout()\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract 3 cluster of data using K-Means. Now let\u0027s use the scaled data\n\nfrom sklearn.cluster import KMeans\n\nkmeans \u003d KMeans(n_clusters\u003d3)\n\npred \u003d kmeans.fit_predict(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]])\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the VpVs versus AI using the previously run K-means algorithm\n\nfig, (ax1, ax2) \u003d plt.subplots(1, 2, figsize\u003d(10, 5))\n\nplt.subplot(1, 2, 1)\n\nax1 \u003d plt.scatter(w5_logs_scaled[\"AI\"],w5_logs_scaled[\"VpVs\"], color\u003d\u0027gray\u0027, edgecolor\u003d\u0027black\u0027, lw\u003d0.3, s\u003d30)\nplt.xlabel(\"AI\",fontsize\u003d14)\nplt.ylabel(r\u0027$\\frac{Vp}{Vs}$\u0027,fontsize\u003d14)\nplt.title(\u0027Raw Data\u0027,fontsize\u003d14)\n\n\nplt.subplot(1, 2, 2)\n\nax2 \u003d plt.scatter(w5_logs_scaled[\"AI\"],w5_logs_scaled[\"VpVs\"], c\u003dpred, edgecolor\u003d\u0027black\u0027, lw\u003d0.3, s\u003d30,cmap\u003d\"prism\")\nplt.xlabel(\"AI\",fontsize\u003d14)\nplt.ylabel(r\u0027$\\frac{Vp}{Vs}$\u0027,fontsize\u003d14)\nplt.title(\u0027K-Means\u0027,fontsize\u003d14)\nplt.tight_layout()\nplt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Define a plotting function\n\nBecause we will be exploring several clustering methods which result requires the same plot, it\u0027s a good idea to define a plotting function"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a function to plot clustering results\n\ndef format_plot(title\u003d\"\",pred\u003d\"\"):\n    \n    fig, (ax1, ax2) \u003d plt.subplots(1, 2, figsize\u003d(10, 5))\n\n    plt.subplot(1, 2, 1)\n    \n    ax1 \u003d plt.scatter(w5_logs_scaled[\"AI\"],w5_logs_scaled[\"VpVs\"], color\u003d\u0027gray\u0027, edgecolor\u003d\u0027black\u0027, lw\u003d0.3, s\u003d30)\n    plt.xlabel(\"AI\",fontsize\u003d14)\n    plt.ylabel(r\u0027$\\frac{Vp}{Vs}$\u0027,fontsize\u003d14)\n    plt.title(\u0027Raw Data\u0027,fontsize\u003d14)\n\n\n    plt.subplot(1, 2, 2)\n\n    ax2 \u003d plt.scatter(w5_logs_scaled[\"AI\"],w5_logs_scaled[\"VpVs\"], c\u003dpred, edgecolor\u003d\u0027black\u0027, lw\u003d0.3, s\u003d30,cmap\u003d\"prism\")\n    plt.xlabel(\"AI\",fontsize\u003d14)\n    plt.ylabel(r\u0027$\\frac{Vp}{Vs}$\u0027,fontsize\u003d14)\n    plt.title(title,fontsize\u003d14)\n    plt.tight_layout()      \n    plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Identify the Optimun Number of Clusters - Elbow Method"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# We can use the elbow plot to decide about the best number of classifications\n\nfrom yellowbrick.cluster import KElbowVisualizer\n\nmodel \u003d KMeans()\n\nelbow_vis \u003d KElbowVisualizer(model, k\u003d(2,11), size\u003d(500, 400), locate_elbow\u003dTrue, timings\u003dFalse)\n "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit the data to the visualizer\n\nelbow_vis.fit(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]]) "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Draw the elbow plot\n\nelbow_vis.show()   "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Distortion**: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Good solutions aren’t those with the lowest score, but rather the ones where you notice a more or less abrupt discontinuity in the descent of the score (even just a change in the slope). \n\nA discontinuity means that the algorithm found some noticeable structural difference when looking for that number of\nclusters in the data. In this example, a good solution seems to be four clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Clustering Analysis Using Gaussian Mixture Models\n\nSo instead of using a distance-based model (K-means), we will now use a distribution-based model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Cluster the log data using a Gaussian Mixture Model (GMM)\n\n# First, train the gaussian model \n\nfrom sklearn.mixture import GaussianMixture\n\ngmm \u003d GaussianMixture(n_components\u003d5).fit(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]])\n\n\n# Generate predictions from the gaussian mixture model \n\npred \u003d gmm.predict(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]])\n\n\n# Plot the result using the format_plot function\n\nformat_plot(\"Gaussian Mixture Model\",pred)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Clustering Analysis Using the Hierarchical Method\n\nHierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. \n\nThe AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n\n - **Ward linkage** minimizes the sum of squared differences within all clusters\n\n - **Complete linkage** minimizes the maximum distance between observations of pairs of clusters\n \n - **Average linkage** minimizes the average of the distances between all observations of pairs of clusters\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n\n# Perform clustering analysis using ward linkage criteria\n\nward \u003d AgglomerativeClustering(n_clusters\u003d3, linkage\u003d\"ward\")\nclust_ward \u003d ward.fit_predict(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]])\n\n# \u0027clust_ward\u0027 is a numpy array, explore it!\n\n\n# Plot clustering results using the format_plot function\n\nformat_plot(\"Hierarchical- Ward Linkage\", clust_ward)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform clustering analysis using complete linkage criteria\n\ncomplete \u003d AgglomerativeClustering(n_clusters\u003d3, linkage\u003d\"complete\")\nclust_complete\u003d complete.fit_predict(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]])\n\n# \u0027clust_complete\u0027 is a numpy array, explore it!\n\n\n# Plot clustering results using the format_plot function\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform clustering analysis using average linkage criteria\n\n\n\n# \u0027clust_average\u0027 is a numpy array, explore it!\n\n\n# Plot clustering results using the format_plot function\n\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9: Clustering Analysis Using DBSCAN Method\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise (i.e., outliers). \n\nThe main idea behind DBSCAN is that a data value belongs to a cluster if it is close to many data values from that cluster"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn import cluster\n\n# Train the dbscan model\n\ndbscan \u003d cluster.DBSCAN(eps\u003d0.09, min_samples\u003d3)\n\n\n# Generate predictions using the dbscan model \n\ndb_pred \u003d dbscan.fit_predict(w5_logs_scaled[[\u0027VpVs\u0027, \u0027AI\u0027]])\n\n\n# Plot clustering results using the format_plot function\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# Note that DBSCAN does not require specifying the number of clusters, let\u0027s review it (\u0027db_pred\u0027)\n\n\n# Noisy data points are given the label -1. They do not belong to any cluster"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Part 2: Prediction of Production Patterns Using Clustering Analysis\n\nIn the second part of this tutorial, we will learn how to train a clustering model using the production data from four wells, and then use this model to predict the production trends from another well"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Read and display the production data from Dataiku\n\nmydataset \u003d dataiku.Dataset(\"production_data\")\nproduction \u003d mydataset.get_dataframe()\n\n# Make sure that the column \u0027Prod Date\u0027 is treated as date\n\nproduction[\"Prod Date\"]\u003d pd.to_datetime(production[\"Prod Date\"])\n\nproduction"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Set the column \u0027Prod Date\u0027 as index\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Print the production data to visualize changes\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Investigate the total number of wells in the dataset\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "# Create a new dataframe that contains only the production data from well \u0027w1\u0027, call it \u0027production_test\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a new dataframe that contains the production data from the rest of the wells (\u0027w2\u0027, \u0027w3\u0027 and \u0027w4\u0027), call it \u0027production_train\u0027\n\nproduction_train\u003d production.query(\u0027Unformatted_UWI!\u003d\"w1\"\u0027)\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Verify the wells contained in each dataframe (\u0027production_test\u0027 \u0026 \u0027production_train\u0027)\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print the production data to visualize changes\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Now let\u0027s plot the monthly gas production versus date for the train wells  \n\nproduction_train.groupby(\"Unformatted_UWI\")[\"Monthly Gas (E3m3)\"].plot(marker\u003d\u0027.\u0027, markersize\u003d5, \n                                                                       legend\u003dTrue, figsize\u003d(8, 6),\n                                                         title\u003d\u0027Monthly Gas Production - Train wells\u0027)\n\nplt.xlabel(\u0027Production date\u0027)\nplt.ylabel(\u0027Monthly Gas Production (e3m3)\u0027)\nplt.show()\n\n# Note that the production from the three wells begins at different times"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Plot the entire production data without discretizing it by well \n\nproduction_train.reset_index()[\"Monthly Gas (E3m3)\"].plot(marker\u003d\u0027.\u0027, markersize\u003d5, \n                                                                       legend\u003dTrue, figsize\u003d(8, 6),\n                                                         title\u003d\u0027Monthly Gas Production - Train wells | Combined\u0027)\n\nplt.xlabel(\u0027Month\u0027)\nplt.ylabel(\u0027Monthly Gas Production (e3m3)\u0027)\nplt.show()\n\n# Note that the \"production peaks\" correspond to the order in which the wells appear in the dataset (w3 -\u003e w2 -\u003e w4) "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now that we have combined the production patterns from the train wells, the next step will be to train a model to see if we can separate different parts of the time series using another feature. For this, we can create a new feature using the .diff() function.\n\n.diff() function is used to calculate the difference between values in an array along with a given axis"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a function to add a new feature to the production dataset, call it \u0027preprocess_data\u0027\n\ndef preprocess_data(df):\n    d\u003d df.reset_index()[[\"Monthly Gas (E3m3)\"]]\n    d[\"diff\"]\u003dd.diff()\n    d.dropna(inplace\u003dTrue)\n    return d"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Apply the \u0027preprocess_data\u0027 function to the production test and train dataframes\n\nproduction_train_processed\u003d preprocess_data(production_train)\n\nproduction_test_processed\u003d preprocess_data(production_test)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the resulting train dataframe \u0027production_train_processed\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the resulting test dataframe \u0027production_test_processed\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Plot the production and diff variables in the same graph\n\nproduction_train_processed.plot(marker\u003d\u0027.\u0027, markersize\u003d5, legend\u003dTrue, figsize\u003d(8, 6),\n                                                         title\u003d\u0027Gas Production \u0026 diff - Train wells | Combined\u0027)\n\nplt.xlabel(\u0027Month\u0027)\nplt.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "# Use the elbow plot on the train dataset to identify the optimum number of clusters\n\nfrom yellowbrick.cluster import KElbowVisualizer\n\nmodel \u003d KMeans(random_state\u003d42)\nelb_visualizer \u003d KElbowVisualizer(model, k\u003d(2,11), size\u003d(600, 400), locate_elbow\u003dTrue, timings\u003dFalse)\nelb_visualizer.fit(production_train_processed)\nelb_visualizer.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Train a K-means clustering model using the production data on the train set (w2, w3, w4)\n\nd\u003dproduction_train_processed\nkmeans \u003d KMeans(n_clusters\u003d3)\nprod_train_pred \u003d kmeans.fit_predict(d)\n\n# \u0027prod_train_pred\u0027 is a numpy array, explore it!\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot the results of the clustering on the train set (w2, w3, w4)\n\nplt.figure(figsize\u003d(8, 6));\nplt.scatter(d.index, d[\"Monthly Gas (E3m3)\"], label \u003d \u0027prod_train_pred\u0027, \n            c\u003dprod_train_pred, edgecolor\u003d\u0027black\u0027, lw\u003d0.5, s\u003d50, cmap\u003dplt.get_cmap(\u0027prism\u0027))\n\nplt.title(\u0027Production trends | Train wells\u0027)\nplt.xlabel(\u0027Month\u0027)\nplt.ylabel(\u0027Monthly Gas Production (e3m3)\u0027)\nplt.legend(prod_train_pred) \nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Implement the previously trained K-means model to predit the production trend on the test set (w1)\n\nd\u003dproduction_test_processed\nprod_test_pred \u003d kmeans.predict(d)\n\n\n# Plot the results of the clustering on the test set (w1)\n\nplt.figure(figsize\u003d(8, 6));\nplt.scatter(d.index, d[\"Monthly Gas (E3m3)\"], linestyle\u003d\u0027solid\u0027, marker\u003d\"o\",  label \u003d \u0027prod_train_pred\u0027, \n            c\u003dprod_test_pred, edgecolor\u003d\u0027black\u0027, lw\u003d0.5, s\u003d50, cmap\u003dplt.get_cmap(\u0027prism\u0027))\n\nplt.title(\u0027W1 - Predicted Production\u0027)\nplt.xlabel(\u0027Month\u0027)\nplt.ylabel(\u0027Monthly Gas Production (e3m3)\u0027)\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "# Now let\u0027s plot the actual production data for well \u0027w1\u0027 and compare it with the predicted production\n\nproduction_test.plot(marker\u003d\u0027o\u0027, markersize\u003d 7, \n                     legend\u003dTrue, figsize\u003d(8, 6),\n                     title\u003d\u0027W1 - Actual Production\u0027)\n\nplt.xlabel(\u0027Production date\u0027)\nplt.ylabel(\u0027Monthly Gas Production (e3m3)\u0027)\nplt.show()"
      ],
      "outputs": []
    }
  ]
}