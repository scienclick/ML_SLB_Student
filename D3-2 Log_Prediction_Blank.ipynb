{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-fund-d3",
      "display_name": "Python (env Fund-d3)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "ashamsa@slb.com",
    "createdOn": 1663181748330,
    "tags": [],
    "customFields": {},
    "modifiedBy": "dbecerra6@slb.com",
    "versionNumber": 2
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel: `Fund-d3`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 3 - Tutorial 2: Supervised Learning\n\nThe objective of this excercise is to learn how to apply supervised learning to predict an output variable from multi-dimensional data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Import requiered libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\nimport dataiku\nfrom dataiku import pandasutils as pdu"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 1- Quick Look at the Data:\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Load and display the well logs dataset (\u0027w5.csv\u0027) and assign \u0027DEPTH\u0027 as the index\n\nmydataset \u003d dataiku.Dataset(\"w5\")\nlog_data \u003d mydataset.get_dataframe()\n\nlog_data \u003d log_data.set_index(\u0027DEPTH\u0027)\n\nlog_data\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Display a summary of the dataset \n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Compute summary statistics of the variables in the dataset\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Compute and draw the histograms for all variables in the dataset\n\nlog_data.hist(bins\u003d50, figsize\u003d(8,8), grid\u003dFalse, color\u003d\u0027green\u0027, edgecolor \u003d \"black\", lw\u003d0.1)\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Generate a correlation matrix of all variables in the dataset\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Plot a heat map using the previously generated person coefficient values from the correlation matrix \n\nsns.heatmap(log_data.corr(), annot\u003dTrue)\n\nplt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Describe the correlation coefficients between the variable \u0027pressure\u0027 and the rest of the variables in the dataset\n\nlog_data.corr()[\"Pressure\"].sort_values(ascending\u003dFalse)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Extract four variables or features and generate a scatter matrix plot\n\nfeatures \u003d [\"Pressure\", \"AI\", \"VpVs\", \"phid\"]\n\nscatter_matrix(log_data[features], figsize\u003d(10, 10), diagonal \u003d \u0027hist\u0027, \n               hist_kwds\u003d{\u0027bins\u0027:50, \u0027color\u0027:\u0027green\u0027},\n               marker\u003d\u0027.\u0027, color\u003d\u0027green\u0027)\nplt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 2- Create the Features and Target Dataframes:\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a new dataframe that only contains the following logs: \u0027AI\u0027,\u0027VpVs\u0027, and \u0027phid\u0027, call it \u0027features\u0027\n\nfeatures\u003d log_data[[\"AI\",\"VpVs\",\"phid\"]]\n\nfeatures"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new dataframe that only contains the variable \u0027pressure\u0027 (our target feature), call it \u0027target\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Split the \u0027features\u0027 and \u0027target\u0027 dataframes into random train and test subsets [80:20 ratio]\n\nfrom sklearn.model_selection import train_test_split \n\nfeatures_train, features_test, target_train, target_test \u003d train_test_split(features, target, test_size\u003d0.2, random_state\u003d1)\n\n# \u0027test_size\u003d0.2\u0027 represents the proportion of datapoints that will be in the test set (20%)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Let\u0027s explore the shape of the \u0027features_train\u0027 and \u0027features_test\u0027 dataframes\n\nprint (\u0027features_train\u0027, features_train.shape)\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 3 - Training and Testing a Random Forest Model\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Define the random forest model\n\nrf\u003d RandomForestRegressor()\n\n\n# Fit the model on the train dataset\n\nrf.fit(features_train, target_train)\n\n\n# Now, we can predict the \u0027target\u0027 data using the trained model\n\ntarget_pred \u003d rf.predict(features_test)\n\n\n# Check the accuracy of predicted data by using MSE and RMSE metrics\n\nmse \u003d mean_squared_error(target_test, target_pred)\nrmse \u003d np.sqrt(mse)\n\nprint (\"MSE: \", mse)\n\nprint (\"RMMSE: \", rmse)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.metrics import r2_score\n\n# Compute the r2 score (coefficient of determination) to evaluate the performance of the model\n\nr2_score(target_test, target_pred)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a prediction error plot for the random forest model\n\nfrom yellowbrick.regressor import prediction_error\n\nvisualizer\u003d prediction_error(RandomForestRegressor(),features_train.values,\n                             target_train.values,features_test.values,target_test.values,\n                            alpha\u003d0.3, size\u003d(400, 400))\n\n# y \u003d actual \u0027pressure\u0027\n# y^ \u003d predicted \u0027pressure\u0027"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 4 - Training and Testing a Support Vector Regression Model\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVR\n\n# First, let\u0027s standardize the data -\u003e Define the scaler \nsc\u003d StandardScaler()\n\n# Fit the scaler to the dataset\nsc.fit(features_train)\n\n# Define the SVR model \nsvm \u003d SVR()\n\n# Fit the model on the train dataset. Don\u0027t forget to call sc.transform () to perform the standarization\nsvm.fit(sc.transform(features_train), target_train.values.ravel())\n\n# Now, we can predict the \u0027target\u0027 data using the trained model\ntarget_pred_svm \u003d svm.predict(sc.transform(features_test))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Compute the r2 score (coefficient of determination) to evaluate the performance of the SVR model\n\nr2_score(target_test, target_pred_svm)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a prediction error plot for the Support Vector Regression (SVR) Model\n\nvisualizer\u003dprediction_error(SVR(),sc.transform(features_train),target_train.values.ravel(),\n                            sc.transform(features_test),target_test.values,\n                           alpha\u003d0.3, size\u003d(500, 400))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 5 - Hyperparameter Tuning Using Grid Search\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Define a dictionary with the hyperparameters of the random forest model that we would like to test\n\n# Let\u0027s try 6 combinations (3×2) of hyperparameters\n\nparam_grid \u003d [{\u0027n_estimators\u0027: [3, 10, 30], \n               \u0027max_features\u0027: [2, 3]}]\n\n\n# n_estimators: number of trees in the forest  \n\n# max_features: maximum number of features to consider when looking for the best split"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n\n#Define the random forest regressor model\n\nforest_reg \u003d RandomForestRegressor()\n\n\n# Now let’s create an object of GridSearchCV\n\ngrid_search \u003d GridSearchCV(forest_reg, param_grid, cv\u003d5, scoring\u003d\u0027r2\u0027,return_train_score\u003dTrue)\n\n\n# Fit the data into the GridSearchCV object\n\ngrid_search.fit(features_train, target_train.values.flatten())\n\n\n# We can now extract the best parameters after tuning\n\nprint(grid_search.best_estimator_)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Now we can predict the \u0027target\u0027 data using the optimal model\n\ntarget_opt_pred \u003d grid_search.predict(features_test)\n\n\n\n# Extract the score of the best estimator model on the testing data\n\nr2_score(target_test, target_opt_pred)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create and display a dataframe with the results from the hyperparameter tuning (grid search)\n\npd.DataFrame(grid_search.cv_results_)\n\n# Note that we have 6 rows: 3 choices for \u0027n_estimators\u0027 times 2 choices for \u0027max_features\u0027"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 6 - Hyperparameter Tuning Using Randomnized Search\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\n\n# Generate a dictionary with the hyperparameters of the random forest model that we would like to test\n\nparam_distribs \u003d {\u0027n_estimators\u0027: randint(1,200),\n                    \u0027max_features\u0027: randint(1, 3)}\n\n\n# Define the random forest regressor model\nforest_reg \u003d RandomForestRegressor(random_state\u003d42)\n\n# Note that random_state sets a seed (42) to ensure that the splits are reproducible\n\n\n# Create an object of RandomizedSearchCV\nrnd_search \u003d RandomizedSearchCV(forest_reg, param_distributions\u003dparam_distribs,\n                                n_iter\u003d10, cv\u003d5, scoring\u003d\u0027r2\u0027, random_state\u003d42)\n\n# Fit the data into the RandomizedSearchCV object\nrnd_search.fit(features_train, target_train.values.flatten())\n\n# Extract the best parameters after tuning\nprint(rnd_search.best_estimator_)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Now we can predict the \u0027target\u0027 data using the optimal model\ntarget_opt_pred_rnd \u003d rnd_search.predict(features_test)\n\n\n# Extract the score of the best estimator model on the testing data\nr2_score(target_test, target_opt_pred_rnd)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create and display a dataframe with the results from the hyperparameter tuning (randomnized search)\n\npd.DataFrame(rnd_search.cv_results_)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 7 - Set Up a Machine Learning Pipeline\u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Define the following steps in the pipeline: deal mith NaN, standardize, model\n\npipe \u003d Pipeline([\n    (\u0027imputer\u0027, SimpleImputer(strategy\u003d\"median\")),\n    (\u0027scaler\u0027, StandardScaler()),\n    (\u0027model\u0027,SVR())\n])\n\n## The pipeline can be used as any other estimator\n\n# First, fit the pipeline to the dataset\npipe.fit(features_train, target_train)\n\n\n# Now, we can predict the \u0027target\u0027 using the trained model\npred_pipe_svr \u003d pipe.predict(features_test)\n\n\n# Compute the r2 score to evaluate the performance of the model\nr2_score(target_test, pred_pipe_svr)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cfont color\u003d\"Red\" size\u003d4\u003e\u003cb\u003eStep 8 - Set Up a ML Pipeline with Grid Search \u003c/font\u003e\u003c/b\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true
      },
      "source": [
        "# Let\u0027s start by checking the list of available parameters\n\npipe.get_params().keys()\n\npipe.get_params()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Define a dictionary with the parameters from the SVR model that we would like to test\n\nparam_grid \u003d [\n    {\u0027model__C\u0027: [1, 3, 10], \n     \u0027model__epsilon\u0027: [.1, .5, .8, 1],\n     \u0027scaler\u0027:[StandardScaler(),MinMaxScaler()]},\n]\n\n# in SVR, C is a regularisation parameter and epsilon defines a margin of tolerance (error sensitivity)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "source": [
        "# Now let’s create an object of GridSearchCV\n\ngrid_search_pipe \u003d GridSearchCV(pipe, param_grid, cv\u003d5,\n                                scoring\u003d\u0027r2\u0027,return_train_score\u003dTrue)\n\n\n# Fit the data into the GridSearchCV object\n\ngrid_search_pipe.fit(features_train, target_train)\n\n\n# We can now extract the best parameters after tuning\n\nprint(grid_search_pipe.best_estimator_)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "source": [
        "# Now, let\u0027s predict the \u0027target\u0027 using the best pipeline model\n\npred_ \u003d grid_search_pipe.predict(features_test)\n\n\n# Compute the r2 score to evaluate the performance of the pipeline model\n\nr2_score(target_test, pred_)"
      ],
      "outputs": []
    }
  ]
}