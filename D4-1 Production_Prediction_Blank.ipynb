{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-ml_foundation",
      "display_name": "Python (env ML_Foundation)",
      "language": "python"
    },
    "creator": "ashamsa@slb.com",
    "createdOn": 1663181947742,
    "tags": [],
    "customFields": {},
    "modifiedBy": "dbecerra6@slb.com",
    "versionNumber": 1,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel `Fund-d4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 4 - Tutorial 1: Predicting Production Data\n\nThe objective of this excercise is to learn how to apply supervised learning to predict an output variable from multi-dimensional data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Import required libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nimport dataiku\nfrom dataiku import pandasutils as pdu\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Peparation"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Read the well dataset \n\nmydataset \u003d dataiku.Dataset(\"well_data_raw\")\nwell_data \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Explore the content of the well_data dataframe\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a scatter plot to display the well location (lat \u0026 long), total depth (tvd), and cum gas production\n\nwell_data.plot(kind\u003d\u0027scatter\u0027, x\u003d\"longitude\", y\u003d\"latitude\", \n        figsize\u003d(7, 5),\n        s\u003dwell_data[\"cum_12_gas_prod\"] / 70, c\u003d\"tvd\",\n        cmap\u003dplt.get_cmap(\"jet\"), \n        alpha\u003d.4, linewidth\u003d2,\n        vmin\u003d3100, vmax\u003d3900)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Display and analyze the distribution of missing values in the dataset\n\nimport missingno as msno\n\nmsno.matrix(well_data)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Another plot to visualize the total number of missing values per column is the bar plot. Let\u0027s display it\n\nmsno.bar(well_data)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a copy of the dataset (well_data), call it \u0027well_data_clean\u0027 and perform the following operations:\n\n# Set \u0027well_id\u0027 as the index \n# Remove the rows where \u0027fluid\u0027\u003d \u0027oil\u0027\n# Remove missing values\n# Remove the column \u0027fluid\u0027\n# Add a new column \u0027target_bin\u0027 and define 5 bins for the \u0027cum_gas_prod\u0027 [\"a\",\"b\", \"c\",\"d\", \"e\"]\n# Make sure to asign the data type in the \u0027target_bin\u0027 column as object\n\nwell_data_clean \u003d (well_data\n        .copy(deep\u003dTrue)\n        .set_index(\"well_id\")\n        .pipe(lambda well_data_: well_data_[~well_data_.fluid.str.contains(\"Oil\", na\u003dTrue)])\n        .dropna()\n        .drop([\"fluid\"], axis\u003d1)\n        .assign(target_bin\u003dlambda well_data_: pd.qcut(well_data_.cum_12_gas_prod, 5, labels\u003d [\"a\",\"b\", \"c\",\"d\", \"e\"]))\n        .assign(target_bin\u003dlambda well_data_: well_data_.target_bin.astype(object))\n               \n)\n\n\n# Let\u0027s print the new dataset\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let\u0027s print the count of values in each bin (\"target_bin\")\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Standardization"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# First create a subset of the \u0027well_data_clean\u0027 dataframe to include only the numerical data\n\nslice_2_scale\u003d well_data_clean[well_data_clean.select_dtypes(include\u003d\u0027number\u0027).columns]\n\n\n# Standardize the dataset and add back the non-numerical column (\u0027target_bin\u0027)\n\nwell_data_clean[well_data_clean.select_dtypes(include\u003d\u0027number\u0027).columns]\u003d StandardScaler().fit_transform(slice_2_scale)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Display the standardized dataset \u0027well_data_clean\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Reshape the DataFrame"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Change the DataFrame format from wide to long using the .melt () function \n\nwell_data_melt \u003d pd.melt(well_data_clean, value_vars\u003d list(well_data_clean.columns).remove(\"target_bin\"), id_vars \u003d\u0027target_bin\u0027)\n\n\n# Visualize the melted DataFrame. Analyze the content of rows and columns\n\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Statistical Graphics Plotting"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import seaborn as sns\n\n# Create a violin plot to visualize the distribution of all variables in the dataset (\u0027well_data_melt\u0027)\n\nplt.figure(figsize\u003d (15, 6))\nplt.xticks(rotation\u003d 90)\nsns.set(font_scale\u003d1)\nsns.set(style\u003d\"whitegrid\")\nsns.violinplot(data\u003d well_data_melt, x\u003d\"variable\", y\u003d\"value\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Create a swarmplot and color the points according to the \u0027target_bin\u0027 variable\n\nplt.figure(figsize\u003d(35, 15))\nplt.xticks(rotation\u003d90)\nsns.set(font_scale \u003d 1)\nsns.set(style\u003d\"whitegrid\")\nsns.swarmplot(data\u003dwell_data_melt, x\u003d\"variable\",y\u003d\"value\",size\u003d5, hue\u003d\"target_bin\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5:  Dimensionality Reduction"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Define a list of variables to be used as features\n\nfeatures \u003d [\u0027fluid_per_stimulated_length\u0027, \u0027proppant_fluid_ratio\u0027,\n           \u0027proppant_per_stage\u0027, \u0027proppant_per_stimulated_length\u0027, \u0027stage_length\u0027,\n           \u0027stimulated_length\u0027, \u0027latitude\u0027, \u0027longitude\u0027, \u0027tvd\u0027,\u0027month_on_production\u0027]\n\n\n# Define the gas production as the target variable \u0027cum_12_gas_prod\u0027, call it \u0027target\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import umap\n\n# UMAP\u003d Uniform Manifold Approximation and Projection\n\n# First, define the umap reducer\n\nreducer \u003d umap.UMAP(random_state\u003d42)\n\n\n# Train the reducer to learn from the features\n\nembedding_umap\u003d reducer.fit_transform(StandardScaler().fit_transform(well_data_clean[features]))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Now let\u0027s display the 2-dimensional representation of the data using Umap\n\nplt.figure(figsize\u003d(15, 8))\nplt.subplot(121)\nplt.scatter(embedding_umap[:, 0], embedding_umap[:, 1], \n            marker\u003d\"o\", s\u003d100, edgecolor\u003d\"k\",\n            c\u003dwell_data_clean[target], \n            cmap\u003d\"prism\", alpha\u003d1)\n\nplt.title(\"UMAP\", fontsize\u003d\"large\")\nplt.xlabel(\"UMAP Dimension 1\")\nplt.ylabel(\"UMAP Dimension 2\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Below are additional tools for visualizing high-dimensional data:\n\n#from sklearn.manifold import TSNE\n#from phate import PHATE\n\n#embedding_tsne \u003d TSNE().fit_transform(StandardScaler().fit_transform(well_data_clean[features]))\n\n#embedding_phate \u003d PHATE(random_state\u003d42).fit_transform(StandardScaler().fit_transform(well_data_clean[features]))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6:  Cross Validation for ML Models\n\nCross-validation is a statistical method used for assessing the effectiveness of machine learning models"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n\n# Split the full dataset (\u0027well_data\u0027) into two parts [30: 70] [test:train]\n\nfeatures_train,features_test,target_train,target_test\u003dtrain_test_split(well_data[features],\n                                               well_data[target],\n                                               test_size\u003d0.3,\n                                               random_state\u003d42)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from lightgbm import LGBMRegressor\nfrom yellowbrick.model_selection import CVScores\nfrom sklearn.model_selection import KFold\n\n# Create a cross-validation strategy. Define the number of folds (k\u003d5) to split the data\n\ncv \u003d KFold(n_splits\u003d5)\n\n\n# Define the classification model using a LightGBM Classifierand visualizer\n\nclass_model\u003d LGBMRegressor()\n\nvisualizer \u003d CVScores(class_model, cv\u003dcv, scoring\u003d\u0027r2\u0027,size\u003d(500,300))\n\n\n# Fit the data to the visualizer\n\nvisualizer.fit(well_data_clean[features], well_data_clean[target])\n\nvisualizer.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from yellowbrick.regressor import PredictionError\n\n# Generate a prediction error plot to evaluate the LGBM model\n\n\u0027LGMB : Light Gradient Boosting Machine\u0027\n\nvisualizer \u003d PredictionError(LGBMRegressor(), size\u003d(500,500))\nvisualizer.fit(features_train, target_train)\nvisualizer.score(features_test, target_test)\nvisualizer.show()\n\n\u0027y \u003d Actual Target Value\u0027\n\u0027ŷ \u003d Predicted Target value\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from xgboost import XGBRegressor\n\n# Now let\u0027s generate the prediction error plot for the XGBoost model\n\n\u0027XGBoost: Extreme Gradient Boosting\u0027\n\nvisualizer \u003d PredictionError(XGBRegressor(), size\u003d(500,500))\nvisualizer.fit(features_train, target_train)\nvisualizer.score(features_test, target_test)\nvisualizer.show()\n\n\n\u0027y \u003d Actual Target Value\u0027\n\u0027ŷ \u003d Predicted Target value\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from yellowbrick.regressor import ResidualsPlot\n\n# Now let\u0027s create a residual plot for the LGBM model\n\nvisualizer \u003d ResidualsPlot(LGBMRegressor(), size\u003d(600,400))\nvisualizer.fit(features_train, target_train)\nvisualizer.score(features_test, target_test)\nvisualizer.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from yellowbrick.model_selection import LearningCurve\n\n# Create an evenly spaced array that will be used as \n\nsizes \u003d np.linspace(start\u003d0.1, stop\u003d1.0, num\u003d50)\n\n\n# Create the learning curve visualizer\n\nlc_viz \u003d LearningCurve(LGBMRegressor(), train_sizes\u003dsizes, \n                    scoring\u003d\u0027r2\u0027,cv\u003dcv,size\u003d(500,500))\n\n\n# Fit the learning curve to our well data \n\nlc_viz.fit(well_data[features], well_data[target])\nlc_viz.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Feature Importances\n\nThe feature engineering process involves selecting the minimum required features to produce a valid model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from yellowbrick.model_selection.importances import FeatureImportances\nfrom lightgbm import LGBMRegressor\n\n# Title case the features for a better display\n\nlabels \u003d list(map(lambda s: s.title(), features))\n\n\n# Define the visualizer\n\nvisualizer \u003d FeatureImportances(LGBMRegressor(), labels\u003dlabels, \n                         relative\u003dFalse, size\u003d(500,500),cv\u003dcv)\n\n# Fit and show the feature importances plot\n\nvisualizer.fit(features_train, target_train)\n\nvisualizer.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom xgboost import XGBRegressor\n\n\u0027RFECV: Recursive feature elimination with cross-validation \u0027\n\n# First, split the clean dataset (\u0027well_data_clean\u0027) into two parts [30: 70] [test:train]\n\nfeatures_train,features_test,target_train,target_test\u003dtrain_test_split(well_data_clean[features],well_data_clean[target],test_size\u003d0.3,random_state\u003d42)\n\n\n# Define the estimator model using XGB\n\nmodel\u003d XGBRegressor()\n\n\n# Create a cross-validation strategy\n\ncv \u003d KFold(n_splits\u003d3)\n\n\n# Define the parameters in the RFE model\n\nrfecv\u003d RFECV(estimator\u003d model, step\u003d1, cv\u003dcv, scoring\u003d \u0027r2\u0027, min_features_to_select\u003d1)\n\n\n# Fit the data to the RFE model\n\nrfecv.fit(features_train, target_train)\n         \n\n# Examine the number of features selected \nprint(\"Optimal number of features: \",  rfecv.n_features_)\n\n\n# Examine the features selected \nprint(\"Selected features: \",  rfecv.support_)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Examine the cross validation scores as more features are selected in the model (remember that we used step\u003d1)\n\nrfecv.grid_scores_"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let\u0027s plot the number of features versus the cross-validation scores\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize\u003d(6, 4))\nplt.title(\u0027Recursive Feature Elimination with Cross-Validation\u0027, fontsize\u003d15, fontweight\u003d\u0027bold\u0027, pad\u003d20)\nplt.xlabel(\u0027Number of features selected\u0027, fontsize\u003d14, labelpad\u003d10)\nplt.ylabel(\u0027Cross validation score (r2)\u0027, fontsize\u003d14, labelpad\u003d10)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, linewidth\u003d2)\nplt.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Now let\u0027s use the output of the RFE model and display the features importances\n\n\n# Create a Dataframe including the optimal features based on RFE \n\ndset \u003d pd.DataFrame()\ndset[\u0027feature\u0027] \u003d features_train.columns[rfecv.support_]\ndset[\u0027importance\u0027] \u003d rfecv.estimator_.feature_importances_\ndset \u003d dset.sort_values(by\u003d\u0027importance\u0027, ascending\u003dFalse)\n\n\n# Plot feature importance\n\nplt.figure(figsize\u003d(8, 8))\nplt.barh(y\u003ddset[\u0027feature\u0027], width\u003ddset[\u0027importance\u0027])\nplt.title(\u0027RFECV - Feature Importances\u0027, fontsize\u003d20, fontweight\u003d\u0027bold\u0027, pad\u003d20)\nplt.xlabel(\u0027Importance\u0027, fontsize\u003d16, labelpad\u003d10)\nplt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Use SHAP Values to Explain How a ML Model Works\n\nSHAP: SHapley Additive exPlanations\n\nSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we\u0027d make if that feature took some baseline value\n\nPositive SHAP value means positive impact on prediction and negative SHAP value means negative impact"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import shap\n\n# Train a XGBoost model\n\nreg\u003d XGBRegressor().fit((well_data[features]), (well_data[target]))\n\n\n# Compute SHAP values\n\nexplainer \u003d shap.Explainer(reg)\nshap_values \u003d explainer((well_data[features]))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate a beeswarm plot\n\nshap.plots.beeswarm(shap_values)\n\n\n# Note that features are also ordered by their effect on prediction\n# Each point represents a row from the dataset\n# The colors represent the feature values, not to be confuseds with the shap values. If the value of a feture is high -\u003e pink"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Generate a Waterfall plot for the observation # 200\n\ni \u003d 200\nshap.waterfall_plot(shap_values[i])\n\n# This plot shows the effect that each feature has on the prediction, for a given observation"
      ],
      "outputs": []
    }
  ]
}